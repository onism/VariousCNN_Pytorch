{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_SENT_EMB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fEEPuJcez-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "045135ca-a4de-453f-a690-14b63925e0e6"
      },
      "source": [
        "%tensorflow_version 1.x \n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from glob import glob\n",
        "nltk.download('punkt')\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgA7BjjSfguD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # download the SNLI, MNLI,,, dataset and pretrained BERT model\n",
        "# !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "# !unzip uncased_L-12_H-768_A-12.zip\n",
        "# !wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "# !unzip snli_1.0.zip\n",
        "# !wget https://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip\n",
        "# !unzip multinli_1.0.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE18nXnyfwIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/gaphex/bert_experimental\n",
        "# !git clone https://github.com/brmson/dataset-sts\n",
        "\n",
        "sys.path.insert(0, 'bert_experimental')\n",
        "sys.path.insert(0, 'dataset-sts/pysts')\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.bert_layer import BertLayer\n",
        "from bert_experimental.finetuning.modeling import BertConfig, BertModel, build_bert_module\n",
        "\n",
        "from loader import load_sts, load_sick2014"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJGoZxscijZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def load_snli(fpaths):\n",
        "  sa, sb, lb = [], [], [] \n",
        "  fpaths = np.atleast_1d(fpaths)\n",
        "  for fpath in fpaths:\n",
        "    with open(fpath) as fi:\n",
        "      for line in fi:\n",
        "        sample = json.loads(line)\n",
        "        sa.append(sample['sentence1'])\n",
        "        sb.append(sample['sentence2'])\n",
        "        lb.append(sample['gold_label'])\n",
        "  return sa, sb, lb\n",
        "\n",
        "def prepare_snli(sa, sb, lb):\n",
        "  classes = {'entailment', 'contradiction'}\n",
        "  anc_to_pairs = defaultdict(list)\n",
        "  filtered = {}\n",
        "  skipped = 0\n",
        "  anchor_id = 0\n",
        "  for xa, xb, y in zip(sa, sb, lb):\n",
        "    anc_to_pairs[xa].append((xb, y))\n",
        "  \n",
        "  for anchor, payload in anc_to_pairs.items():\n",
        "    filtered[anchor_id] = defaultdict(list)\n",
        "    filtered[anchor_id]['anchor'].append(anchor)\n",
        "    labels = set([t[1] for t in payload])\n",
        "    if len(labels&classes) == len(classes):\n",
        "      for text, label in payload:\n",
        "        filtered[anchor_id][label].append(text)\n",
        "      anchor_id += 1\n",
        "    else:\n",
        "      skipped += 1\n",
        "  print('loaded:{}\\n Skipped:{}'.format(anchor_id, skipped))\n",
        "  return filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsLnbQpEkqDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1aa67c94-f100-4f1c-d4d5-96271e7c2b11"
      },
      "source": [
        "train_data = [\"./snli_1.0/snli_1.0_train.jsonl\", \"./multinli_1.0/multinli_1.0_train.jsonl\"]\n",
        "test_data = [\"./snli_1.0/snli_1.0_test.jsonl\", \"./multinli_1.0/multinli_1.0_dev_matched.jsonl\"]\n",
        "\n",
        "tr_a, tr_b, tr_l = load_snli(train_data)\n",
        "ts_a, ts_b, ts_l = load_snli(test_data)\n",
        "\n",
        "fd_tr = prepare_snli(tr_a, tr_b, tr_l)\n",
        "fd_ts = prepare_snli(ts_a, ts_b, ts_l)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded:277277\n",
            " Skipped:1603\n",
            "loaded:5853\n",
            " Skipped:804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSh7Y-eBlcQw",
        "colab_type": "text"
      },
      "source": [
        "For training the model we will sample triplets, consisting of an anchor, a positive \n",
        "sample and a negative sample.\n",
        "To handle complex batch generation logic we use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxMbNThXlgPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletGenerator:\n",
        "    def __init__(self, datadict, hard_frac = 0.5, batch_size=256):\n",
        "        self.datadict = datadict\n",
        "        self._anchor_idx = np.array(list(self.datadict.keys()))\n",
        "        self._hard_frac = hard_frac\n",
        "        self._generator = self.generate_batch(batch_size)\n",
        "\n",
        "    def generate_batch(self, size):\n",
        "        while True:\n",
        "\n",
        "            hards = int(size*self._hard_frac)\n",
        "            anchor_ids = np.array(np.random.choice(self._anchor_idx, size, replace=False))\n",
        "\n",
        "            anchors = self.get_anchors(anchor_ids)\n",
        "            positives = self.get_positives(anchor_ids)\n",
        "            negatives = np.hstack([self.get_hard_negatives(anchor_ids[:hards]),\n",
        "                                   self.get_random_negatives(anchor_ids[hards:])])\n",
        "            labels = np.ones((size,1))\n",
        "\n",
        "            assert len(anchors) == len(positives) == len(negatives) == len(labels) == size\n",
        "\n",
        "            yield [anchors, positives, negatives], labels\n",
        "            \n",
        "    def get_anchors(self, anchor_ids):\n",
        "        classes = ['anchor']\n",
        "        samples = self.get_samples_from_ids(anchor_ids, classes)\n",
        "        return samples\n",
        "    \n",
        "    def get_positives(self, anchor_ids):\n",
        "        classes = ['entailment']\n",
        "        samples = self.get_samples_from_ids(anchor_ids, classes)\n",
        "        return samples\n",
        "\n",
        "    def get_hard_negatives(self, anchor_ids):\n",
        "        classes = ['contradiction']\n",
        "        samples = self.get_samples_from_ids(anchor_ids, classes)\n",
        "        return samples\n",
        "\n",
        "    def get_random_negatives(self, anchor_ids):\n",
        "        samples = []\n",
        "        classes = ['contradiction', 'neutral','entailment']\n",
        "        for anchor_id in anchor_ids:\n",
        "\n",
        "            other_anchor_id = self.get_random(self._anchor_idx, anchor_id)\n",
        "            avail_classes = list(set(self.datadict[other_anchor_id].keys()) & set(classes))\n",
        "            sample_class = self.get_random(avail_classes)\n",
        "            sample = self.get_random(self.datadict[other_anchor_id][sample_class])\n",
        "            samples.append(sample)\n",
        "        samples = np.array(samples)\n",
        "        return samples\n",
        "    \n",
        "    def get_samples_from_ids(self, anchor_ids, classes):\n",
        "        samples = []\n",
        "        for anchor_id in anchor_ids:\n",
        "            sample_class = self.get_random(classes)\n",
        "            sample = self.get_random(self.datadict[anchor_id][sample_class])\n",
        "            samples.append(sample)\n",
        "        samples = np.array(samples)\n",
        "        return samples\n",
        "\n",
        "    @staticmethod\n",
        "    def get_random(seq, exc=None):\n",
        "        if len(seq) == 1:\n",
        "            return seq[0]\n",
        "                                      \n",
        "        selected = None\n",
        "        while selected is None or selected == exc:\n",
        "            selected = np.random.choice(seq)\n",
        "        return selected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuoSK66Olu8d",
        "colab_type": "text"
      },
      "source": [
        "Batch anchor IDs are selected randomly from all available IDs.\n",
        "Anchor samples are retrieved from anchor samples of their IDs.\n",
        "Positive samples are retrieved from entailment samples of their IDs.\n",
        "Negative samples are retrieved from contradiction samples of their IDs. These may be considered hard negative samples, because they are often semantically similar to their anchors. To reduce overfitting we mix them with random negative samples retrieved from other, random ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4VSVZkcl25A",
        "colab_type": "text"
      },
      "source": [
        "We can frame the problem of learning a measure of sentence similarity as a ranking problem. Suppose we have a corpus of k paraphrase sentence pairs x and y and want to learn a function that\n",
        "estimates if y is a paraphrase(解释;释义;意译) of x or not.\n",
        "\n",
        "For some x we have a single positive sample y and k-1 negative samples y_k. This probability distirbution can be written as \n",
        "\n",
        "p(y|x) = \\frac{P(x,y)}{\\sum P(x, y_k}\n",
        "\n",
        "The joint probability of P(x,y) is estimated using a scoring function, S\n",
        "\n",
        "P(x,y) \\approx e^{S(x,y)}\n",
        "\n",
        "We will be minimizing the negative log probability of our data,\n",
        "So, for a batch of K triplets for the loss we can write down\n",
        "\n",
        "L(x,y,\\theta) = \\frac{1}{K} \\sum log(P(y_i|x_i)) \\approx \n",
        "\\frac{1}{K} \\sum < log \\sum e^{S} - S  >\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiatGIVololl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_loss(vectors):\n",
        "  anc, pos, neg = vectors\n",
        "  pos_sim = tf.reduce_sum((anc*pos), axis=-1, keepdims=True)\n",
        "  neg_mul = tf.matmul(anc, neg, transpose_b=True)\n",
        "  neg_sim = tf.log(tf.reduce_sum( tf.exp(neg_mul), axis=-1, keepdims=True  ))\n",
        "  loss = tf.nn.relu(neg_sim - pos_sim)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJGK7viqoZSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_DIR = \"/content/uncased_L-12_H-768_A-12/\" #@param {type:\"string\"}\n",
        "\n",
        "build_bert_module(BERT_DIR+\"bert_config.json\",\n",
        "                  BERT_DIR+\"vocab.txt\",\n",
        "                  BERT_DIR+\"bert_model.ckpt\", \n",
        "                  \"bert_module\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQC0ndXao2U1",
        "colab_type": "text"
      },
      "source": [
        "The model has three inputs for the anchor, postive and negative samples. A BERT layer with a mean pooling operation is used as a shared text encoder. Text preprocessing is handled automatically by the layer. \n",
        "\n",
        "For convenience, we create 3 keras models: enc_model for encoding sentences, sim_model for compute similarity between sentence pairs and trn_model for training. All models use shared weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlPgl6O_ovzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dot_product(tensor_pair):\n",
        "  u, v = tensor_pair\n",
        "  return tf.reduce_sum( (u * v), axis=-1, keepdims=True )\n",
        "\n",
        "def consine_similarity(tensor_pair):\n",
        "  u, v = tensor_pair\n",
        "  u = tf.math.l2_normalize(u, axis=-1)\n",
        "  v = tf.math.l2_normalize(v, axis=-1)\n",
        "  return tf.reduce_sum(( u* v), axis=-1, keepdims=True) \n",
        "\n",
        "def mean_loss(y_true, y_pred):\n",
        "  mean_pred = tf.reduce_mean(y_pred - 0 * y_true)\n",
        "  return mean_pred\n",
        "\n",
        "def build_model(module_path, seq_len=24, tune_lr=6, loss=softmax_loss):\n",
        "  inp_anc = tf.keras.Input(shape=(1,), dtype=tf.string, name='input_anchor')\n",
        "  inp_pos = tf.keras.Input(shape=(1,), dtype=tf.string, name='input_pos')\n",
        "  inp_neg = tf.keras.Input(shape=(1,), dtype=tf.string, name='input_neg')\n",
        "  sent_encoder = BertLayer(module_path, seq_len, n_tune_layers=tune_lr, do_preprocessing=True,\n",
        "                           verbose=False, pooling='mean', trainable=True, tune_embeddings=False)\n",
        "  c = 0.5 # avoid Nan loss \n",
        "  anc_enc = c * sent_encoder(inp_anc)\n",
        "  pos_enc = c * sent_encoder(inp_pos)\n",
        "  neg_enc = c * sent_encoder(inp_neg)\n",
        "\n",
        "  loss = tf.keras.layers.Lambda(loss, name='loss')([anc_enc, pos_enc, neg_enc])\n",
        "  sim = tf.keras.layers.Lambda(consine_similarity, name='sim')([anc_enc, pos_enc])\n",
        "\n",
        "  trn_model = tf.keras.models.Model(inputs=[inp_anc, inp_pos, inp_neg], outputs=[loss])\n",
        "  enc_model = tf.keras.models.Model(inputs=inp_anc, outputs=[anc_enc])\n",
        "  sim_model = tf.keras.models.Model(inputs=[inp_anc, inp_pos], outputs=[sim])\n",
        "\n",
        "  trn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),loss=mean_loss, metrics=[])\n",
        "  trn_model.summary()\n",
        "  mdict = {\n",
        "        \"enc_model\": enc_model,\n",
        "        \"sim_model\": sim_model,\n",
        "        \"trn_model\": trn_model\n",
        "    }\n",
        "  return mdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruh1MRKDsewj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RankCorrCallback(Callback):\n",
        "\n",
        "    def __init__(self, loader, filepaths, name=None, verbose=False,\n",
        "                 sim_model=None, savemodel=None, savepath=None):\n",
        "\n",
        "        self.savemodel = savemodel\n",
        "        self.savepath = savepath\n",
        "        self.sim_model = sim_model\n",
        "        self.loader = loader\n",
        "        self.verbose = verbose\n",
        "        self.name = name\n",
        "\n",
        "        self.samples, self.labels = self.load_datasets(filepaths)\n",
        "        self.best = defaultdict(int)\n",
        "        super(RankCorrCallback, self).__init__()\n",
        "\n",
        "    def load_datasets(self, filepaths):\n",
        "        _xa, _xb, _y = [], [], [] \n",
        "        for filepath in filepaths:\n",
        "            sa, sb, lb = self.loader(filepath)\n",
        "            sa = self.join_by_whitespace(sa)\n",
        "            sb = self.join_by_whitespace(sb)\n",
        "            _xa += sa\n",
        "            _xb += sb\n",
        "            _y += list(lb)\n",
        "        return [_xa, _xb], _y\n",
        "            \n",
        "    @staticmethod\n",
        "    def join_by_whitespace(list_of_str):\n",
        "        return [\" \".join(s) for s in list_of_str]\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "\n",
        "        pred = self.sim_model.predict(self.samples, batch_size=128, \n",
        "                                      verbose=self.verbose).reshape(-1,)\n",
        "\n",
        "        for metric, func in [(\"spearman_r\", spearmanr),(\"pearson_r\", pearsonr)]:\n",
        "          coef, _ = func(self.labels, pred)\n",
        "          coef = np.round(coef, 4)\n",
        "\n",
        "          metric_name = f\"{self.name}_{metric}\"\n",
        "          message = f\"{metric_name} = {coef}\"\n",
        "          if coef > self.best[metric_name]:\n",
        "            self.best[metric_name] = coef\n",
        "            message = \"*** New best: \" + message\n",
        "            if self.savemodel and self.savepath and metric == \"spearman_r\":\n",
        "                self.savemodel.save_weights(self.savepath)\n",
        "\n",
        "          print(message)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.on_epoch_begin(None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8k1RQNOsrac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "e70a083d-06ee-4432-cbe0-e7a9aa52ad42"
      },
      "source": [
        "model_dict = build_model(module_path=\"bert_module\", tune_lr=4, loss=softmax_loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method BertLayer.call of <bert_experimental.finetuning.bert_layer.BertLayer object at 0x7f61ede32400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method BertLayer.call of <bert_experimental.finetuning.bert_layer.BertLayer object at 0x7f61ede32400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method BertLayer.call of <bert_experimental.finetuning.bert_layer.BertLayer object at 0x7f61ede32400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_anchor (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_pos (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_neg (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          109482240   input_anchor[0][0]               \n",
            "                                                                 input_pos[0][0]                  \n",
            "                                                                 input_neg[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_mul (TensorFlowOpLa [(None, 768)]        0           bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_mul_1 (TensorFlowOp [(None, 768)]        0           bert_layer[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_mul_2 (TensorFlowOp [(None, 768)]        0           bert_layer[2][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "loss (Lambda)                   (None, 1)            0           tf_op_layer_mul[0][0]            \n",
            "                                                                 tf_op_layer_mul_1[0][0]          \n",
            "                                                                 tf_op_layer_mul_2[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 109,482,240\n",
            "Trainable params: 28,351,488\n",
            "Non-trainable params: 81,130,752\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vv0SQK-s0b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HFRAC = 0.5\n",
        "BSIZE = 200\n",
        "\n",
        "enc_model = model_dict[\"enc_model\"]\n",
        "sim_model = model_dict[\"sim_model\"]\n",
        "trn_model = model_dict[\"trn_model\"]\n",
        "\n",
        "tr_gen = TripletGenerator(fd_tr, hard_frac=HFRAC, batch_size=BSIZE)\n",
        "ts_gen = TripletGenerator(fd_ts, hard_frac=HFRAC, batch_size=BSIZE)\n",
        "\n",
        "clb_sts = RankCorrCallback(load_sts, glob(\"./dataset-sts/data/sts/semeval-sts/all/*test*.tsv\"), name='STS',\n",
        "                               sim_model=sim_model, savemodel=enc_model, savepath=\"encoder_en.h5\")\n",
        "clb_sick = RankCorrCallback(load_sick2014, glob(\"./dataset-sts/data/sts/sick2014/SICK_test_annotated.txt\"), \n",
        "                                name='SICK', sim_model=sim_model)\n",
        "\n",
        "callbacks = [clb_sts, clb_sick]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI9dafUFs5v7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9ebddcc1-f645-4c65-b543-e1ad29de91fb"
      },
      "source": [
        "trn_model.fit_generator(\n",
        "    tr_gen._generator, validation_data=ts_gen._generator,\n",
        "    steps_per_epoch=256, validation_steps=32, epochs=10, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** New best: STS_spearman_r = 0.5418\n",
            "*** New best: STS_pearson_r = 0.5475\n",
            "*** New best: SICK_spearman_r = 0.5799\n",
            "*** New best: SICK_pearson_r = 0.6069\n",
            "Epoch 1/10\n",
            " 16/256 [>.............................] - ETA: 9:56 - loss: 2.0933 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXqyG9_ytAak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(trn_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}