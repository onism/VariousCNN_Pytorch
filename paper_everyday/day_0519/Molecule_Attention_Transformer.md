    ref: https://arxiv.org/pdf/2002.08264v1.pdf

# Abstract

Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure.

# Introduction

We adapt Transformer to chemical molecules by augmenting the self-attention with inter-atomic distances and molecular grah structure.

# Related work

# MAT

## Transformer

On a high level, Transformer for classifications has N attention blocks followed by a pooling and a classification layer. Each attention block is composed of a multi-head self-attention layer, followed by a feed-forward block that includes a residual connection and layer normalization.
